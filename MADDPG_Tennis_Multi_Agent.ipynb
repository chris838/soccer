{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb, pickle, torch, random\n",
    "\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "from holoviews.operation.timeseries import rolling\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "from maddpg_agent import MaddpgAgent\n",
    "from trainer import Trainer\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"./Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain and reset env\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# Size of the global state/action space (across all agents)\n",
    "actions = env_info.previous_vector_actions\n",
    "states = env_info.vector_observations\n",
    "global_state_space_size = states.flatten().shape[0]\n",
    "global_action_space_size = actions.flatten().shape[0]\n",
    "print(f\"Global states: {global_state_space_size}\")\n",
    "print(f\"Global actions: {global_action_space_size}\")\n",
    "\n",
    "# Size of the local state/action space (for each agent individually)\n",
    "action_space_size = brain.vector_action_space_size\n",
    "state_space_size = brain.num_stacked_vector_observations * brain.vector_observation_space_size\n",
    "print(f\"Local states: {state_space_size}\")\n",
    "print(f\"Local actions: {action_space_size}\")\n",
    "\n",
    "# Examine the state space \n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create/load replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the replay buffer\n",
    "replay_buffer_size_max = int(1e6)\n",
    "min_samples_required = 10000\n",
    "replay_buffer = ReplayBuffer(max_size=replay_buffer_size_max, min_samples_required=min_samples_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save replay buffer\n",
    "#pickle.dump( replay_buffer, open( \"replay_buffer.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load replay buffer\n",
    "replay_buffer = pickle.load( open( \"replay_buffer.pickle\", \"rb\" ) )\n",
    "print(f\"Loaded replay buffer with {len(replay_buffer)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "### Create a new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "trainer = Trainer(\n",
    "    env = env,\n",
    "    replay_buffer = replay_buffer,\n",
    "    discount = 0.99,\n",
    "    tau = 0.01,\n",
    "    actor_lr = 1e-4,\n",
    "    critic_lr = 3e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start/resume training sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boost the agent learn rate becuase we're using batch norm\n",
    "trainer.agents[0].actor_lr  = 4 * 1e-4\n",
    "trainer.agents[0].critic_lr = 4 * 3e-4\n",
    "\n",
    "trainer.train(\n",
    "    num_episodes = 20000,\n",
    "    batch_size = 512,\n",
    "    train_every_steps = 4,\n",
    "    noise_level = 0.12,\n",
    "    noise_decay = 0.9999,\n",
    "    max_episode_length = 250,\n",
    "    print_episodes = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display returns\n",
    "max_returns = trainer.get_max_returns()\n",
    "raw_returns = hv.Curve(max_returns, 'Episode', 'Return').relabel('Single episode')\n",
    "smooth_returns = rolling(hv.Curve(\n",
    "    max_returns, 'Episode', 'Return'), rolling_window=100).relabel('100 episode average')\n",
    "max_returns_curve = (raw_returns * smooth_returns).relabel('Max episode return')\n",
    "\n",
    "# Display loss\n",
    "average_loss = trainer.get_average_loss()\n",
    "actor_loss = hv.Curve(average_loss[:,0], 'Training iteration', 'Loss').relabel('Actor')\n",
    "critic_loss = hv.Curve(average_loss[:,1], 'Training iteration', 'Loss').relabel('Critic')\n",
    "loss_curves = (actor_loss * critic_loss).relabel('Actor/critic loss')\n",
    "\n",
    "(max_returns_curve + loss_curves).opts(opts.Curve(axiswise=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/restore training state\n",
    "\n",
    "### Pausing/resuming training progress\n",
    "\n",
    "This is especially useful because the Unity environment handle will be corrupted if you interrupt whilst training. Simply save the trainer, restart the kernel and unity environment, then load your progress to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trainer to disk\n",
    "pickle.dump( trainer, open( \"saved_models/trainer.pickle\", \"wb\" ) )\n",
    "\n",
    "# Save torch params to file\n",
    "for i, agent in enumerate(trainer.agents):\n",
    "    torch.save(agent.actor_optimiser,   f\"saved_models/agent_{i}_actor_optimiser.pt\")\n",
    "    torch.save(agent.critic_optimiser,  f\"saved_models/agent_{i}_critic_optimiser.pt\")\n",
    "    torch.save(agent.actor,         f\"saved_models/agent_{i}_actor_model.pt\")\n",
    "    torch.save(agent.actor_target,  f\"saved_models/agent_{i}_actor_target_model.pt\")\n",
    "    torch.save(agent.critic,        f\"saved_models/agent_{i}_critic_model.pt\")\n",
    "    torch.save(agent.critic_target, f\"saved_models/agent_{i}_critic_target_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainer from disk\n",
    "trainer = pickle.load( open( \"saved_models/trainer.pickle\", \"rb\" ) )\n",
    "\n",
    "# Load torch params from file (NOT safe across refactors)\n",
    "for i, agent in enumerate(trainer.agents):\n",
    "    agent.actor_optimiser  = torch.load(f\"saved_models/agent_{i}_actor_optimiser.pt\")\n",
    "    agent.critic_optimiser = torch.load(f\"saved_models/agent_{i}_critic_optimiser.pt\")\n",
    "    agent.actor         = torch.load(f\"saved_models/agent_{i}_actor_model.pt\")\n",
    "    agent.actor_target  = torch.load(f\"saved_models/agent_{i}_actor_target_model.pt\")\n",
    "    agent.critic        = torch.load(f\"saved_models/agent_{i}_critic_model.pt\")\n",
    "    agent.critic_target = torch.load(f\"saved_models/agent_{i}_critic_target_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch agent play\n",
    "\n",
    "To view random play according to the OU noise process, set the noise level to 1. This is what we use to generate exploratory behaviour initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    t = 0\n",
    "    while True:\n",
    "        t += 1\n",
    "\n",
    "        actions = [agent.act(state, noise_level=0.5) for agent, state in zip(trainer.agents, states)]\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print(f'Episode: {i}; length: {t}, max score: {np.max(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
